# _半监督学习方法—阶梯网络_
## _摘要_
&emsp;&emsp;我们在深度神经网络中将监督学习和非监督学习相结合。所提出的模型在通过反向传播的训练过程中，可以同时最小化监督学习和非监督学习的代价函数，避免了分层预训练的需要。我们的工作建立在Valpola（2015）提出的阶梯网络的基础上，并将此模型与监督学习相结合。本文最终的模型在传统MNIST分类、半监督的MNIST和CIFAR-10分类中均达到了当前最优的水平。
## _简介_
&emsp;&emsp;在本文中，我们介绍了一种非监督学习方法，它非常适合改进监督学习。使用无监督学习来补充监督的想法并不新鲜，Suddarth和Kergosien（1990）提出了一项帮助训练神经网络的技术，即通过在多个任务之间共享隐层表达，使网络有更好的泛化能力。无监督任务有多种选择，例如，在模型的每层重建输入（Ranzato和Szummer，2008）或将每个输入样本分类到其自己的类中（Dosovitskiy等，2014）。  
&emsp;&emsp;尽管一些方法能够同时应用有监督和无监督学习（Ranzato和Szummer，2008; Goodfellow，2013a），但这些无监督的辅助任务通常仅用作预训练，然后是正常的监督学习（例如， Hinton和Salakhutdinov，2006）。在复杂的任务中，输入中的信息通常比可以表示的要多得多，而无监督学习不能知道什么信息对当前的任务有用。例如，考虑应用于自然图像的自动编码机方法：辅助解码器网络尝试从内部表示来重建原始输入。自动编码器将尝试保留在像素级重建图像所需的所有细节，即使不涉及像素值的变换通常对分类问题是无影响的。像素级重建所需的大部分信息都是无关紧要的，而比较相关的不变特征却不能单独重建整个原始输入。
